# Response API Configuration
# Enables OpenAI Response API support with conversation chaining
response_api:
  enabled: true
  store_backend: "memory"  # Options: "memory", "milvus", "redis"
  ttl_seconds: 86400       # 24 hours
  max_responses: 1000

# Router Replay Configuration (System-Level)
# Provides storage backend configuration for router_replay plugin
# Per-decision settings (max_records, capture settings) are configured via router_replay plugin
router_replay:
  store_backend: "memory"  # Options: "memory", "redis", "postgres", "milvus"
  ttl_seconds: 2592000     # 30 days retention (for persistent backends)
  async_writes: false      # Enable async writes for better performance

semantic_cache:
  enabled: true
  backend_type: "memory"  # Options: "memory", "milvus", or "hybrid"
  similarity_threshold: 0.8
  max_entries: 1000  # Only applies to memory backend
  ttl_seconds: 3600
  eviction_policy: "fifo"
  # HNSW index configuration (for memory backend only)
  use_hnsw: true  # Enable HNSW index for faster similarity search
  hnsw_m: 16  # Number of bi-directional links (higher = better recall, more memory)
  hnsw_ef_construction: 200  # Construction parameter (higher = better quality, slower build)

  # Hybrid cache configuration (when backend_type: "hybrid")
  # Combines in-memory HNSW for fast search with Milvus for scalable storage
  # max_memory_entries: 100000 # Max entries in HNSW index (default: 100,000)
  # backend_config_path: "config/milvus.yaml" # Path to Milvus config

  # Embedding model for semantic similarity matching
  # If not specified, automatically uses the model configured in embedding_models section
  # Options: "mmbert" (multilingual, 768-dim), "qwen3" (high quality, 1024-dim, 32K context), "gemma" (balanced, 768-dim, 8K context)
  # embedding_model: "mmbert"  # Optional: explicitly set if you want to override auto-detection

tools:
  enabled: true
  top_k: 3
  similarity_threshold: 0.2
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

prompt_guard:
  enabled: true  # Global default - can be overridden per category with jailbreak_enabled
  use_mmbert_32k: true
  model_id: "models/mmbert32k-jailbreak-detector-merged"
  jailbreak_mapping_path: "models/mmbert32k-jailbreak-detector-merged/jailbreak_type_mapping.json"
  threshold: 0.7
  use_cpu: true

# Classifier configuration
classifier:
  category_model:
    model_id: "models/mmbert32k-intent-classifier-merged"
    use_mmbert_32k: true
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/mmbert32k-intent-classifier-merged/category_mapping.json"
  pii_model:
    model_id: "models/mmbert32k-pii-detector-merged"
    use_mmbert_32k: true
    threshold: 0.9
    use_cpu: true
    pii_mapping_path: "models/mmbert32k-pii-detector-merged/pii_type_mapping.json"

# Hallucination mitigation configuration
# Disabled by default - enable in decisions via hallucination plugin
hallucination_mitigation:
  enabled: false
  # Fact-check classifier: determines if a prompt needs fact verification
  fact_check_model:
    model_id: "models/mmbert32k-factcheck-classifier-merged"
    threshold: 0.6
    use_cpu: true
    use_mmbert_32k: true
  # Hallucination detector: verifies if LLM response is grounded in context
  hallucination_model:
    model_id: "models/mom-halugate-detector"
    threshold: 0.8
    use_cpu: true
    # False positive reduction settings
    min_span_length: 2  # Minimum tokens in a span to report (filters single-token false positives)
    min_span_confidence: 0.6  # Minimum confidence for a span to be reported
    context_window_size: 50  # Characters of context around flagged spans
    enable_nli_filtering: true  # Use NLI to filter false positives
    nli_entailment_threshold: 0.75  # Filter spans with high entailment scores
  # NLI model: provides explanations for hallucinated spans
  nli_model:
    model_id: "models/mom-halugate-explainer"
    threshold: 0.9
    use_cpu: true

# Feedback detector configuration
# Classifies user feedback into 4 types: satisfied, need_clarification, wrong_answer, want_different
feedback_detector:
  enabled: true
  model_id: "models/mmbert32k-feedback-detector-merged"
  threshold: 0.7
  use_cpu: true
  use_mmbert_32k: true

# External models configuration
# Used for advanced routing signals like preference-based routing via external LLM
# Users can configure external models in their config to enable these features
# Example:
# external_models:
#   - llm_provider: "vllm"
#     model_role: "preference"
#     llm_endpoint:
#       address: "127.0.0.1"
#       port: 8000
#     llm_model_name: "openai/gpt-oss-120b"
#     llm_timeout_seconds: 30
#     parser_type: "json"
#     access_key: ""  # Optional: for Authorization header (Bearer token)

# Embedding Models Configuration
# This is the UNIFIED configuration for all embedding-related features:
# - Semantic Cache: Automatically uses the configured model
# - Tool Selection: Uses the configured model for tool matching
# - Embedding Signal: Uses the model specified in hnsw_config.model_type
# - Complexity Signal: Uses the model specified in hnsw_config.model_type
#
# Available models:
# - Qwen3-Embedding-0.6B (Pro): Up to 32K context, high quality, 1024-dim
# - EmbeddingGemma-300M (Flash): Up to 8K context, fast inference, Matryoshka support (768/512/256/128)
# - mmBERT-Embed-32K-2D-Matryoshka (Ultra): Up to 32K context, 1800+ languages, 2D Matryoshka (layer early exit + dimension reduction)
embedding_models:
  # qwen3_model_path: "models/mom-embedding-pro"
  # gemma_model_path: "models/mom-embedding-flash"
  mmbert_model_path: "models/mom-embedding-ultra"
  use_cpu: true  # Set to false for GPU acceleration (requires CUDA)
  # HNSW Configuration
  # Improves performance by preloading candidate embeddings at startup
  # and using HNSW index for O(log n) similarity search
  hnsw_config:
    model_type: "mmbert"         # Which model to use: "qwen3" (high quality), "gemma" (fast), or "mmbert" (multilingual)
    preload_embeddings: true    # Precompute candidate embeddings at startup
    target_dimension: 768        # Embedding dimension (1024 for qwen3, 768 for gemma/mmbert)
    # For mmbert only: target_layer (3/6/11/22) for layer early exit
    enable_soft_matching: true
    min_score_threshold: 0.5

# Observability Configuration
observability:
  metrics:
    enabled: true  # Enable Prometheus /metrics endpoint for vllm-sr serve
  tracing:
    enabled: true  # Enable distributed tracing for vllm-sr serve
    provider: "opentelemetry"  # Provider: opentelemetry, openinference, openllmetry
    exporter:
      type: "otlp"  # Export spans to Jaeger (via OTLP gRPC)
      endpoint: "vllm-sr-jaeger:4317"  # Jaeger collector (use container name for vllm-sr serve)
      insecure: true  # Use insecure connection (no TLS)
    sampling:
      type: "always_on"  # Sampling: always_on, always_off, probabilistic
      rate: 1.0  # Sampling rate for probabilistic (0.0-1.0)
    resource:
      service_name: "vllm-sr"
      service_version: "v0.1.0"
      deployment_environment: "development"

# Looper Configuration
# The looper handles multi-model orchestration for decisions with multiple model_refs
# It makes HTTP calls through Envoy to execute algorithms like confidence routing
# NOTE: Looper requests include "x-vsr-looper-request: true" header, so ExtProc skips
#       plugin processing to avoid infinite loops
looper:
  enabled: true  # Enable looper for multi-model decisions
  # Endpoint points to Envoy (same container), which handles load balancing and auth
  # Port should match listener port (default: 8888)
  endpoint: "http://localhost:8899/v1/chat/completions"
  timeout_seconds: 120  # Timeout in seconds for each model call
  headers: {}  # Optional headers (e.g., {"Authorization": "Bearer xxx"})

clear_route_cache: true

# Model Selection Configuration
# ML-based algorithms for intelligent model routing within decisions
# Reference: https://github.com/ulab-uiuc/LLMRouter
model_selection:
  enabled: true
  # Default algorithm: knn, kmeans, svm
  default_algorithm: "knn"

  # LLM candidates configuration
  llm_candidates_path: "config/model_selection/llm_candidates.json"

  # Pre-trained models path (contains saved model weights)
  models_path: "models/model_selection"

  # Training data path (for custom training)
  training_data_path: "config/model_selection/routing_training_data.jsonl"

  # Algorithm-specific configurations
  knn:
    k: 5  # Number of neighbors
    weights: "distance"  # "uniform" or "distance"
    metric: "cosine"  # "cosine", "euclidean", "manhattan"
    # Pre-trained model file (relative to models_path)
    model_file: "knn_model.json"

  kmeans:
    num_clusters: 8
    efficiency_weight: 0.3  # Balance between quality (0) and efficiency (1)
    max_iterations: 100
    model_file: "kmeans_model.json"

  svm:
    kernel: "rbf"  # "linear", "rbf", "poly"
    c: 1.0  # Regularization parameter
    gamma: "auto"  # Kernel coefficient
    model_file: "svm_model.json"

  # Custom training options
  # Allows customers to train on their own models
  custom_training:
    enabled: false
    # Path to customer-provided training data
    custom_data_path: ""
    # Whether to merge with pre-trained data or replace
    merge_with_pretrained: true
    # Minimum training samples required
    min_samples: 50

# Law domain cache embedding configuration
#
# This config defines how to train a law-domain LoRA adapter
# for semantic caching.

domain: law
description: "Legal queries, case law, contracts, and legal reasoning"

# Data source (to be created)
data_file: "data/cache_embeddings/law/unlabeled_queries.jsonl"
queries_count: 60000  # Target: ~60k legal queries

# Output paths
output_dir: "models/law-cache-lora"

# HuggingFace repository (optional - for automatic upload)
hf_repo: "your-org/semantic-router-law-cache"

# Training parameters (same as medical for consistency)
vllm_model: "Qwen/Qwen2.5-7B-Instruct"  # Better for legal reasoning than 1.5B
base_model: "sentence-transformers/all-MiniLM-L12-v2"
paraphrases: 3
negatives: 2
batch_size: 32
learning_rate: 2e-5
temperature: 0.05
epochs: 1

# AWS configuration
aws_instance_type: "g5.12xlarge"
aws_gpus: 4
estimated_time_hours: 3

# Data sources (for reference)
datasets:
  - name: "CaseHOLD"
    source: "HuggingFace: casehold"
    size: "53k legal holdings"
    license: "CC BY 4.0"

  - name: "LegalQA (StackExchange Law)"
    source: "Archive.org or StackExchange dumps"
    size: "~30k Q&A pairs"
    license: "CC BY-SA 4.0"

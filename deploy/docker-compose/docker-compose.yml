services:

  # Semantic Router External Processor Service
  semantic-router:
    image: ghcr.io/vllm-project/semantic-router/extproc:latest
    container_name: semantic-router
    ports:
      - "50051:50051"
    volumes:
      - ../../config:/app/config:ro
      - ../../models:/app/models:ro
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - LD_LIBRARY_PATH=/app/lib
      # Use main config by default; override via CONFIG_FILE if needed
      - CONFIG_FILE=${CONFIG_FILE:-/app/config/config.yaml}
      # Optional informational envs (router reads YAML for tracing config)
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OTEL_SERVICE_NAME=vllm-semantic-router
      - HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface
      - HF_HUB_ENABLE_HF_TRANSFER=1
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # Envoy Proxy Service
  envoy:
    image: envoyproxy/envoy:v1.31.7
    container_name: envoy-proxy
    ports:
      - "8801:8801"  # Main proxy port
      - "19000:19000"  # Admin interface
    volumes:
      - ./addons/envoy.yaml:/etc/envoy/envoy.yaml:ro
    command: ["/usr/local/bin/envoy", "-c", "/etc/envoy/envoy.yaml", "--component-log-level", "ext_proc:trace,router:trace,http:trace"]
    depends_on:
      semantic-router:
        condition: service_healthy
    networks:
      - semantic-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19000/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Mock vLLM service for testing profile
  mock-vllm:
    build:
      context: ../../tools/mock-vllm
      dockerfile: Dockerfile
    container_name: mock-vllm
    profiles: ["testing"]
    ports:
      - "8000:8000"
    networks:
      semantic-network:
        ipv4_address: 172.28.0.10
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s

  # Jaeger for distributed tracing (OTLP gRPC + UI)
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "4318:4317"   # OTLP gRPC (mapped to 4318 on host to avoid conflicts)
      - "16686:16686" # Web UI
    networks:
      - semantic-network

  # Prometheus and Grafana for observability
  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    volumes:
      - ./addons/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro
      - prometheus-data:/prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yaml
      - --storage.tsdb.retention.time=15d
    ports:
      - "9090:9090"
    networks:
      - semantic-network

  grafana:
    image: grafana/grafana:11.5.1
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - PROMETHEUS_URL=prometheus:9090
    ports:
      - "3000:3000"
    volumes:
      - ./addons/grafana.ini:/etc/grafana/grafana.ini:ro
      - ./addons/grafana-datasource.yaml:/etc/grafana/provisioning/datasources/datasource.yaml:ro
      - ./addons/grafana-datasource-jaeger.yaml:/etc/grafana/provisioning/datasources/datasource_jaeger.yaml:ro
      - ./addons/grafana-dashboard.yaml:/etc/grafana/provisioning/dashboards/dashboard.yaml:ro
      - ./addons/llm-router-dashboard.json:/etc/grafana/provisioning/dashboards/llm-router-dashboard.json:ro
      - grafana-data:/var/lib/grafana
    networks:
      - semantic-network
    depends_on:
      - prometheus

  # Open WebUI (kept for feature parity and user choice)
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3001:8080"  # Expose Open WebUI on host 3001
    environment:
      - WEBUI_NAME=Open WebUI
      # Route Open WebUI's OpenAI-compatible calls through Pipelines by default
      - OPENAI_API_BASE_URL=http://pipelines:9099
      - OPENAI_API_KEY=0p3n-w3bu!
    volumes:
      - openwebui-data:/app/backend/data
    networks:
      - semantic-network

  # Chat UI (Hugging Face) replacing Open WebUI
  chat-ui:
    image: ghcr.io/huggingface/chat-ui-db:latest
    container_name: chat-ui
    ports:
      - "3002:3000"  # Expose Chat UI on host 3002
    environment:
      # Point Chat UI to Envoy's OpenAI-compatible endpoint
      - OPENAI_BASE_URL=http://envoy-proxy:8801/v1
      # Provide a token if your upstream requires it; for HF router use hf_xxx
      - OPENAI_API_KEY=${OPENAI_API_KEY:-changeme}
      # MongoDB for persistence (use Atlas by overriding MONGODB_URL)
      - MONGODB_URL=${MONGODB_URL:-mongodb://mongo:27017}
      - MONGODB_DB_NAME=${MONGODB_DB_NAME:-chat-ui}
      # Optional theming (override as needed)
      - PUBLIC_APP_NAME=${PUBLIC_APP_NAME:-HuggingChat}
      - PUBLIC_APP_ASSETS=${PUBLIC_APP_ASSETS:-chatui}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    depends_on:
      envoy:
        condition: service_started
      semantic-router:
        condition: service_healthy
      mongo:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - semantic-network

  # Open WebUI Pipelines server (executes Python pipelines)
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      # Persistent pipelines storage (auto-loaded on start)
      - openwebui-pipelines:/app/pipelines
      # Mount our vLLM Semantic Router pipeline
      - ./addons/vllm_semantic_router_pipe.py:/app/pipelines/vllm_semantic_router_pipe.py:ro
    networks:
      - semantic-network

  # MongoDB for Chat UI persistence (dev default; use Atlas in prod)
  mongo:
    image: mongo:7
    container_name: mongo
    restart: unless-stopped
    volumes:
      - mongo-data:/data/db
    networks:
      - semantic-network

  # LLM Katan service for testing
  llm-katan:
    image: ${LLM_KATAN_IMAGE:-ghcr.io/vllm-project/semantic-router/llm-katan:latest}
    container_name: llm-katan
    profiles: ["testing", "llm-katan"]
    ports:
      - "8002:8002"
    environment:
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-}
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - ../../models:/app/models:ro
      - hf-cache:/home/llmkatan/.cache/huggingface
    networks:
      semantic-network:
        ipv4_address: 172.28.0.20
    command: ["llm-katan", "--model", "/app/models/Qwen/Qwen3-0.6B", "--served-model-name", "qwen3", "--host", "0.0.0.0", "--port", "8002"]
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8002/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Semantic Router Dashboard
  dashboard:
    # Use pre-built image from GHCR, fallback to local build for development
    # image: ${DASHBOARD_IMAGE:-ghcr.io/vllm-project/semantic-router/dashboard:latest}
    build:
      context: ../../
      dockerfile: dashboard/backend/Dockerfile
    container_name: semantic-router-dashboard
    command: ["/app/dashboard-backend", "-port=8700", "-static=/app/frontend", "-config=/app/config/config.yaml"]
    environment:
      - DASHBOARD_PORT=8700
      - TARGET_GRAFANA_URL=http://grafana:3000
      - TARGET_PROMETHEUS_URL=http://prometheus:9090
      - TARGET_JAEGER_URL=http://jaeger:16686
      - TARGET_ROUTER_API_URL=http://semantic-router:8080
      - TARGET_ROUTER_METRICS_URL=http://semantic-router:9190/metrics
      - TARGET_OPENWEBUI_URL=http://openwebui:8080
      - TARGET_CHATUI_URL=http://chat-ui:3000
      - ROUTER_CONFIG_PATH=/app/config/config.yaml
    volumes:
      - ../../config:/app/config:rw
    ports:
      - "8700:8700"
    networks:
      - semantic-network
    depends_on:
      semantic-router:
        condition: service_healthy
      grafana:
        condition: service_started
      prometheus:
        condition: service_started
      chat-ui:
        condition: service_started
      openwebui:
        condition: service_started
      pipelines:
        condition: service_started
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8700/healthz"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

networks:
  semantic-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

volumes:
  models-cache:
    driver: local
  prometheus-data:
  grafana-data:
  mongo-data:
  openwebui-data:
  openwebui-pipelines:
  hf-cache:

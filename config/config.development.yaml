# Development Configuration Example with Stdout Tracing
# This configuration enables distributed tracing with stdout exporter
# for local development and debugging.

bert_model:
  model_id: models/all-MiniLM-L12-v2
  threshold: 0.6
  use_cpu: true

semantic_cache:
  enabled: true
  backend_type: "memory"
  similarity_threshold: 0.8
  max_entries: 100
  ttl_seconds: 600
  eviction_policy: "fifo"

tools:
  enabled: false
  top_k: 3
  similarity_threshold: 0.2
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

prompt_guard:
  enabled: false

vllm_endpoints:
  - name: "local-endpoint"
    address: "127.0.0.1"
    port: 8000
    weight: 1

model_config:
  "test-model":
    pii_policy:
      allow_by_default: true

classifier:
  category_model:
    model_id: "models/category_classifier_modernbert-base_model"
    use_modernbert: true
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"

categories:
  - name: test
    system_prompt: "You are a test assistant."
    model_scores:
      - model: test-model
        score: 1.0
        use_reasoning: false

default_model: test-model

# Auto model name for automatic model selection (optional)
# Uncomment and set to customize the model name for automatic routing
# auto_model_name: "MoM"

api:
  batch_classification:
    max_batch_size: 10
    metrics:
      enabled: true

# Observability Configuration - Development with Stdout
observability:
  tracing:
    # Enable tracing for development/debugging
    enabled: true
    
    # OpenTelemetry provider
    provider: "opentelemetry"
    
    exporter:
      # Stdout exporter prints traces to console (great for debugging)
      type: "stdout"
      
      # No endpoint needed for stdout
      # endpoint: ""
      # insecure: true
    
    sampling:
      # Always sample in development to see all traces
      type: "always_on"
      
      # Rate not used for always_on
      # rate: 1.0
    
    resource:
      # Service name for trace identification
      service_name: "vllm-semantic-router-dev"
      
      # Version for development
      service_version: "dev"
      
      # Environment identifier
      deployment_environment: "development"
